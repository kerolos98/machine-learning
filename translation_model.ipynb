{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import re\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Embedding,TimeDistributed\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import shuffle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "LSTM_NODES =256\n",
    "NUM_SENTENCES = 30000\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples input: 30000\n",
      "num samples output: 30000\n",
      "num samples output input: 30000\n"
     ]
    }
   ],
   "source": [
    "path='C:\\\\Users\\PC\\Desktop\\data_set/deu.txt'\n",
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "\n",
    "count = 0\n",
    "for line in open(path, encoding=\"utf-8\"):\n",
    "    count += 1\n",
    "\n",
    "    if count > NUM_SENTENCES:\n",
    "        break\n",
    "\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "    input_sentence, output,s = line.rstrip().split('\\t')\n",
    "\n",
    "    output_sentence = output + ' <eos>'\n",
    "    output_sentence_input = '<sos> ' + output\n",
    "\n",
    "    input_sentences.append(input_sentence)\n",
    "    output_sentences.append(output_sentence)\n",
    "    output_sentences_inputs.append(output_sentence_input)\n",
    "\n",
    "print(\"num samples input:\", len(input_sentences))\n",
    "print(\"num samples output:\", len(output_sentences))\n",
    "print(\"num samples output input:\", len(output_sentences_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go away.\n",
      "Gehen Sie weg. <eos>\n",
      "<sos> Gehen Sie weg.\n"
     ]
    }
   ],
   "source": [
    "print(input_sentences[172])\n",
    "print(output_sentences[172])\n",
    "print(output_sentences_inputs[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 4711\n",
      "Length of longest sentence in input: 6\n"
     ]
    }
   ],
   "source": [
    "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Length of longest sentence in input: %g\" % max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the output: 10919\n",
      "Length of longest sentence in the output: 11\n"
     ]
    }
   ],
   "source": [
    "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "output_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_sequences.shape: (30000, 6)\n",
      "encoder_input_sequences[172]: [ 0  0  0  0 23 96]\n"
     ]
    }
   ],
   "source": [
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
    "print(\"encoder_input_sequences[172]:\", encoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_inputs[\"i'm\"])\n",
    "print(word2idx_inputs[\"read\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (30000, 11)\n",
      "decoder_input_sequences[172]: [  2 156   6 147   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
    "print(\"decoder_input_sequences[172]:\", decoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (30000, 11)\n",
      "decoder_input_sequences[172]: [156   6 147   1   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_output_sequences.shape)\n",
    "print(\"decoder_input_sequences[172]:\", decoder_output_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_embeds='D:\\\\glove.6B/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "glove_file = open(path_embeds, encoding=\"utf8\")\n",
    "embeddings_dictionary=dict()\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_SIZE))\n",
    "for word, index in word2idx_inputs.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.6109e-01  3.9902e-01 -1.3540e-01 -2.6579e-01  1.2291e-01  1.2668e+00\n",
      " -3.1164e-01 -2.6861e-01 -4.7820e-02 -3.2642e-01  2.9340e-01  8.5681e-02\n",
      " -2.5868e-01  5.8440e-04  2.0940e-01  1.2185e-01  6.3982e-01  2.3537e-01\n",
      " -5.4758e-01 -2.2416e-01  3.9379e-02 -4.5530e-01 -3.2071e-02 -3.5783e-01\n",
      "  3.6941e-01 -3.7714e-01 -3.9291e-01 -1.7099e-01  5.0699e-01  4.2622e-01\n",
      "  3.1585e-01  4.1306e-01 -7.2180e-02  1.4811e-01 -2.1061e-01  5.5002e-01\n",
      " -1.6419e-01  3.9613e-02  2.2926e-01 -1.8447e-01 -7.0578e-01  1.7350e-01\n",
      " -4.5185e-01 -5.3340e-01 -7.4506e-01 -2.0174e-01 -1.4373e-01 -6.0950e-01\n",
      " -2.2733e-01 -8.0096e-01  9.4654e-01  2.3967e-01  5.8265e-01  5.3151e-01\n",
      " -3.8951e-01 -1.5985e+00 -3.5368e-01  4.6296e-01  1.3289e+00  5.9022e-03\n",
      " -1.1194e-04  1.1528e+00 -6.6395e-01 -1.0022e+00  4.2442e-01  6.9708e-03\n",
      "  7.7096e-01  1.0685e+00 -2.9703e-01  1.9858e-01 -4.6281e-02  6.5891e-01\n",
      "  7.8765e-01 -7.1153e-01  1.7818e-02  8.0047e-01  1.5151e-01  1.1654e-01\n",
      " -3.1582e-01 -6.6401e-01  1.7498e-01 -5.8295e-01 -7.4917e-01 -5.2312e-01\n",
      " -1.4449e+00 -5.9193e-01  3.4345e-02 -1.1250e+00  9.8024e-02  9.9370e-02\n",
      "  3.3389e-01  2.9061e-02  4.2144e-01 -5.1359e-02 -4.8436e-01 -7.2908e-02\n",
      "  2.0406e-01 -4.7103e-01 -2.4160e-01  7.0213e-01]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_dictionary[\"read\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.67670013e-03,  2.57719994e-01,  2.93870002e-01, -4.18220013e-01,\n",
       "        4.64750007e-02,  3.24360013e-01,  4.34249997e-01,  5.83119988e-01,\n",
       "       -8.87579992e-02,  8.93549994e-02, -3.36170010e-02,  5.92159986e-01,\n",
       "        4.99049991e-01,  5.36769986e-01,  2.82660007e-01,  1.45310000e-01,\n",
       "        6.90370023e-01, -3.31779987e-01, -6.68099999e-01,  4.12250012e-01,\n",
       "        2.15289995e-01,  2.97659993e-01,  5.56259990e-01,  1.11950003e-01,\n",
       "       -2.21609999e-03, -1.61520004e-01,  4.24199998e-01, -4.70910013e-01,\n",
       "       -4.03829992e-01,  3.15369993e-01,  7.49130011e-01,  7.35879987e-02,\n",
       "       -2.30289996e-01,  2.79529989e-01,  7.21069992e-01, -1.75830007e-01,\n",
       "        8.85310024e-02,  3.35319996e-01,  2.63390005e-01,  2.56030001e-02,\n",
       "       -8.22130024e-01, -3.81139994e-01,  1.16300002e-01, -3.24680001e-01,\n",
       "        1.77310005e-01,  3.31319988e-01,  2.36330003e-01,  1.08939998e-01,\n",
       "       -2.33480006e-01, -3.63359988e-01,  6.76199973e-01, -7.20240027e-02,\n",
       "        8.88689980e-02,  8.25680017e-01, -6.06790006e-01, -2.00929999e+00,\n",
       "       -9.83650029e-01, -7.24460006e-01,  1.47370005e+00,  7.88850009e-01,\n",
       "       -2.67870009e-01,  1.33809996e+00,  2.64519989e-01,  5.31530023e-01,\n",
       "        8.16779971e-01,  2.04760004e-02,  6.39760017e-01, -4.49439995e-02,\n",
       "        1.01310000e-01, -7.02380016e-02, -1.99890003e-01,  2.48730004e-01,\n",
       "        7.21769989e-01,  4.64860015e-02, -1.29829996e-05, -3.37739997e-02,\n",
       "        1.17440000e-01, -1.54290006e-01, -1.12419999e+00,  2.69820005e-01,\n",
       "        3.76419991e-01,  4.33420002e-01, -8.14620018e-01, -2.42090002e-01,\n",
       "       -1.10080004e+00, -8.51979971e-01,  6.40860021e-01, -3.14379990e-01,\n",
       "       -8.56019974e-01,  1.66710004e-01,  9.51839983e-02, -5.99720001e-01,\n",
       "        1.77080005e-01,  1.09159999e-01, -2.87330002e-01,  1.61290005e-01,\n",
       "       -8.45049977e-01, -9.74500000e-01,  4.62339997e-01, -4.03030008e-01])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 11, 10920)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_one_hot = np.zeros((\n",
    "        len(input_sentences),\n",
    "        max_out_len,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")\n",
    "decoder_targets_one_hot.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(decoder_output_sequences):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len,trainable=False)\n",
    "intial_lstm=LSTM(LSTM_NODES, return_sequences=True)\n",
    "encoder = LSTM(LSTM_NODES, return_state=True)\n",
    "decoder_embedding = Embedding(num_words_output, LSTM_NODES)\n",
    "decoder_lstm = LSTM(LSTM_NODES, return_sequences=True, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "with tf.device('/device:GPU:0'):\n",
    "    encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
    "    X=embedding_layer(encoder_inputs_placeholder)\n",
    "    X1 = Dropout(0.2)(X)\n",
    "    X_encoder, h, c=encoder(X1)\n",
    "    encoder_states = [h, c]\n",
    "    decoder_inputs_placeholder = Input(shape=(max_out_len,))\n",
    "    decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "    X11 = Dropout(0.3)(decoder_outputs)\n",
    "    decoder_outputs = decoder_dense(X11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 6, 100)       471200      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 11)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 6, 100)       0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 11, 256)      2795520     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 365568      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 11, 256), (N 525312      embedding_1[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 11, 256)      0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 11, 10920)    2806440     dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,964,040\n",
      "Trainable params: 6,492,840\n",
      "Non-trainable params: 471,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Model([encoder_inputs_placeholder,\n",
    "  decoder_inputs_placeholder], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27000 samples, validate on 3000 samples\n",
      "Epoch 1/30\n",
      "27000/27000 [==============================] - 106s 4ms/sample - loss: 0.8276 - acc: 0.8625 - val_loss: 1.5393 - val_acc: 0.7782\n",
      "Epoch 2/30\n",
      "27000/27000 [==============================] - 85s 3ms/sample - loss: 0.7095 - acc: 0.8714 - val_loss: 1.5034 - val_acc: 0.7811\n",
      "Epoch 3/30\n",
      "27000/27000 [==============================] - 81s 3ms/sample - loss: 0.6088 - acc: 0.8799 - val_loss: 1.4720 - val_acc: 0.7843\n",
      "Epoch 4/30\n",
      "27000/27000 [==============================] - 80s 3ms/sample - loss: 0.5254 - acc: 0.8876 - val_loss: 1.4726 - val_acc: 0.7852\n",
      "Epoch 5/30\n",
      "27000/27000 [==============================] - 75s 3ms/sample - loss: 0.4590 - acc: 0.8944 - val_loss: 1.4603 - val_acc: 0.7879\n",
      "Epoch 6/30\n",
      "27000/27000 [==============================] - 77s 3ms/sample - loss: 0.4056 - acc: 0.9015 - val_loss: 1.4530 - val_acc: 0.7874\n",
      "Epoch 7/30\n",
      "27000/27000 [==============================] - 83s 3ms/sample - loss: 0.3628 - acc: 0.9087 - val_loss: 1.4510 - val_acc: 0.7888\n",
      "Epoch 8/30\n",
      "27000/27000 [==============================] - 88s 3ms/sample - loss: 0.3298 - acc: 0.9143 - val_loss: 1.4602 - val_acc: 0.7908\n",
      "Epoch 9/30\n",
      "27000/27000 [==============================] - 92s 3ms/sample - loss: 0.3063 - acc: 0.9184 - val_loss: 1.4572 - val_acc: 0.7921\n",
      "Epoch 10/30\n",
      "27000/27000 [==============================] - 83s 3ms/sample - loss: 0.2890 - acc: 0.9212 - val_loss: 1.4725 - val_acc: 0.7897\n",
      "Epoch 11/30\n",
      "27000/27000 [==============================] - 85s 3ms/sample - loss: 0.2737 - acc: 0.9241 - val_loss: 1.4724 - val_acc: 0.7909\n",
      "Epoch 12/30\n",
      "27000/27000 [==============================] - 82s 3ms/sample - loss: 0.2629 - acc: 0.9262 - val_loss: 1.4778 - val_acc: 0.7915\n",
      "Epoch 13/30\n",
      "27000/27000 [==============================] - 79s 3ms/sample - loss: 0.2532 - acc: 0.9280 - val_loss: 1.4928 - val_acc: 0.7911\n",
      "Epoch 14/30\n",
      "27000/27000 [==============================] - 82s 3ms/sample - loss: 0.2442 - acc: 0.9295 - val_loss: 1.4939 - val_acc: 0.7931\n",
      "Epoch 15/30\n",
      "27000/27000 [==============================] - 111s 4ms/sample - loss: 0.2368 - acc: 0.9310 - val_loss: 1.5042 - val_acc: 0.7919\n",
      "Epoch 16/30\n",
      "27000/27000 [==============================] - 101s 4ms/sample - loss: 0.2301 - acc: 0.9323 - val_loss: 1.5064 - val_acc: 0.7909\n",
      "Epoch 17/30\n",
      "27000/27000 [==============================] - 89s 3ms/sample - loss: 0.2240 - acc: 0.9336 - val_loss: 1.5135 - val_acc: 0.7920\n",
      "Epoch 18/30\n",
      "27000/27000 [==============================] - 69s 3ms/sample - loss: 0.2188 - acc: 0.9349 - val_loss: 1.5167 - val_acc: 0.7916\n",
      "Epoch 19/30\n",
      "27000/27000 [==============================] - 62s 2ms/sample - loss: 0.2141 - acc: 0.9357 - val_loss: 1.5215 - val_acc: 0.7919\n",
      "Epoch 20/30\n",
      "27000/27000 [==============================] - 64s 2ms/sample - loss: 0.2084 - acc: 0.9370 - val_loss: 1.5121 - val_acc: 0.7926\n",
      "Epoch 21/30\n",
      "27000/27000 [==============================] - 62s 2ms/sample - loss: 0.2066 - acc: 0.9370 - val_loss: 1.5266 - val_acc: 0.7937\n",
      "Epoch 22/30\n",
      "27000/27000 [==============================] - 88s 3ms/sample - loss: 0.2014 - acc: 0.9382 - val_loss: 1.5314 - val_acc: 0.7937\n",
      "Epoch 23/30\n",
      "27000/27000 [==============================] - 92s 3ms/sample - loss: 0.1982 - acc: 0.9392 - val_loss: 1.5475 - val_acc: 0.7939\n",
      "Epoch 24/30\n",
      "27000/27000 [==============================] - 60s 2ms/sample - loss: 0.1930 - acc: 0.9402 - val_loss: 1.5466 - val_acc: 0.7922\n",
      "Epoch 25/30\n",
      "27000/27000 [==============================] - 77s 3ms/sample - loss: 0.1908 - acc: 0.9409 - val_loss: 1.5456 - val_acc: 0.7918\n",
      "Epoch 26/30\n",
      "27000/27000 [==============================] - 65s 2ms/sample - loss: 0.1886 - acc: 0.9415 - val_loss: 1.5561 - val_acc: 0.7920\n",
      "Epoch 27/30\n",
      "27000/27000 [==============================] - 63s 2ms/sample - loss: 0.1864 - acc: 0.9419 - val_loss: 1.5469 - val_acc: 0.7938\n",
      "Epoch 28/30\n",
      "27000/27000 [==============================] - 64s 2ms/sample - loss: 0.1838 - acc: 0.9421 - val_loss: 1.5563 - val_acc: 0.7916\n",
      "Epoch 29/30\n",
      "27000/27000 [==============================] - 67s 2ms/sample - loss: 0.1827 - acc: 0.9423 - val_loss: 1.5643 - val_acc: 0.7948\n",
      "Epoch 30/30\n",
      "27000/27000 [==============================] - 102s 4ms/sample - loss: 0.1801 - acc: 0.9433 - val_loss: 1.5752 - val_acc: 0.7931\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(\n",
    "    [encoder_input_sequences, decoder_input_sequences],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
    "decoder_states = [h, c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(max_out_len):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "\n",
    "        if idx > 0:\n",
    "            word = idx2word_target[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        target_seq[0, 0] = idx\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(output_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input: It's bitter cold.\n",
      "Response: es ist bitterkalt.\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(input_sentences))\n",
    "input_seq = encoder_input_sequences[i:i+1]\n",
    "translation = translate_sentence(input_seq)\n",
    "print('-')\n",
    "print('Input:', input_sentences[i])\n",
    "print('Response:', translation[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.save('encoding_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.save('decoding_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
